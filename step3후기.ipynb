{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95145aa0-686d-4bd2-acf9-4d05de0fcb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í›ˆë ¨ê³¼ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ê³  ì „ì²˜ë¦¬\n",
    "def get_train_and_test_data(mode):\n",
    "    # ê±´ë¬¼ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
    "    building_info = pd.read_csv('open/building_info.csv').drop(['ESSì €ì¥ìš©ëŸ‰(kWh)', 'PCSìš©ëŸ‰(kW)'], axis=1)\n",
    "    building_info = process_info(building_info)\n",
    "\n",
    "    # í›ˆë ¨ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ë° ì „ì²˜ë¦¬\n",
    "    train_data = pd.read_csv('open/train.csv').drop(['ì¼ì¡°(hr)', 'ì¼ì‚¬(MJ/m2)'], axis=1)\n",
    "    train_data.columns = ['num_date_time', 'building', 'date_time', 'temp', 'prec', 'wind', 'hum', 'target']\n",
    "    train_data = process_data(train_data, mode)\n",
    "\n",
    "    # ê±´ë¬¼ ì •ë³´ ë³‘í•©\n",
    "    test_data = pd.read_csv('open/test.csv')\n",
    "    test_data.columns = ['num_date_time', 'building', 'date_time', 'temp', 'prec', 'wind', 'hum']\n",
    "    test_data = process_data(test_data, mode)\n",
    "\n",
    "    # í†µê³„ì¹˜ ê³„ì‚°\n",
    "    train_data, test_data = mean_std(train_data, test_data, mode)\n",
    "\n",
    "    if mode == 'all' or mode == 'gu_all':\n",
    "        train_data = train_data.merge(building_info, on='building', how='left')\n",
    "        test_data = test_data.merge(building_info, on='building', how='left')\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "def process_info(data, is_train=True):\n",
    "    # ì—´ ì´ë¦„ ë³€ê²½\n",
    "    data.columns = ['building', 'type', 'all_area', 'cool_area', 'sun']\n",
    "    # sun ì—´ì˜ '-'ë¥¼ 0ìœ¼ë¡œ ë³€ê²½í•˜ê³  floatë¡œ í˜•ë³€í™˜\n",
    "    data['sun'] = data['sun'].replace('-', 0).astype('float')\n",
    "\n",
    "    # ê±´ë¬¼ íƒ€ì…ì„ ìˆ«ìë¡œ ë§¤í•‘\n",
    "    value_dict = {value: index for index, value in enumerate(data['type'].unique())}\n",
    "    data['type'] = data['type'].map(value_dict)\n",
    "\n",
    "    filtered_data = data[(data['type'] == 7) & (data['cool_area'] != 0)]\n",
    "    result = (filtered_data['all_area'].iloc[1:].sum() / filtered_data['cool_area'].iloc[1:].sum())\n",
    "    condition = (data['type'] == 7) & (data['cool_area'] == 0)\n",
    "    data.loc[condition, 'cool_area'] = (data.loc[condition, 'all_area'] / result).astype('int')\n",
    "\n",
    "    filtered_data = data[(data['type'] == 9) & (data['cool_area'] > 500)]\n",
    "    result = (filtered_data['all_area'].sum() / filtered_data['cool_area'].sum())\n",
    "    condition = (data['type'] == 9) & (data['cool_area'] < 500)\n",
    "    data.loc[condition, 'cool_area'] = round(data.loc[condition, 'all_area'] / result, 1)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd772fa-c530-4687-a489-e8b36260c689",
   "metadata": {},
   "source": [
    "#### ì½”ë“œë¶„ì„\n",
    " - process_info()\n",
    "     - ë¹Œë”©ì •ë³´ì— íƒœì–‘ê´‘ìš©ëŸ‰(sun)ê²°ì¸¡ì¹˜ 0ìœ¼ë¡œ ì²˜ë¦¬í•˜ê³  ì •ìˆ˜í˜• íƒ€ì…ì„ floatìœ¼ë¡œ ë³€í™˜\n",
    "     - filtered_data, type=7ì¼ë•Œ ëƒ‰ë°©ë©´ì ì´ 0ì´ ì•„ë‹Œ ë°ì´í„°ì— ëŒ€í•´ ì „ì²´ ë©´ì (all_area) ëŒ€ë¹„ ëƒ‰ë°© ë©´ì ì˜ ë¹„ìœ¨ì„ ê³„ì‚°\n",
    "     - condition, type=9ì¼ë•Œ ëƒ‰ë°© ë©´ì ì´ 500ë³´ë‹¤ í° ê²½ìš° ì „ì²´ ë©´ì  ëŒ€ë¹„ ëƒ‰ë°© ë©´ì ì„ ì¡°ì •\n",
    "       -  data.loc[condition, 'cool_area']: conditionì„ ë§Œì¡±í•˜ëŠ” í–‰ë“¤ì˜ cool_areaì—´ì— ì ‘ê·¼\n",
    "       - data.loc[condition, 'all_area'] / result: ì¡°ê±´ ë§Œì¡± í–‰ë“¤ì˜ 'all_area'ì—´ì„ í‰ê·  ë¹„ìœ¨ì¸ resultë¡œ ë‚˜ëˆˆ ê°’\n",
    "       - round(...,1): ì†Œìˆ˜ì  ì²«ì§¸ìë¦¬\n",
    " - get_train_and_test_data()\n",
    "     - ê±´ë¬¼ì •ë³´ì—ì„œ í›ˆë ¨ë°ì´í„°ì™€ ìƒê´€ì´ ì—†ë‹¤ê³  ìƒê°í•œ ESS, PCSëŠ” ì œê±°í•œê²ƒìœ¼ë¡œ ì „ì²˜ë¦¬ì—ì„œ íŒŒì•…, trainì€ testì™€ ë§ì¶”ê¸° ìœ„í•´ ì¼ì¡° ì¼ì‚¬ ì œê±°\n",
    "     - modeì— ë”°ë¼ í‰ê·  ë° í‘œì¤€ í¸ì°¨ ê³„ì‚° í˜¹ì€ ë‹¤ë¥¸ í†µê³„ì¹˜ ê³„ì‚°\n",
    "     - if mode == 'all' or mode == 'gu_all': modeê°€ ë‘˜ì¤‘ í•˜ë‚˜ì¼ ë•Œ building_infoë¥¼ ë³‘í•©\n",
    "     - ë‹¤ë¥¸ modeì—ì„  ê±´ë¬¼ì •ë³´ í™œìš©í•˜ì§€ ì•ŠìŒ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fd760b-5148-4638-b21c-5bc473ce63b6",
   "metadata": {},
   "source": [
    "> modeëŠ” ë°ì´í„° ì „ì²˜ë¦¬ì™€ ê´€ë ¨ëœ ë‹¤ì–‘í•œ ì˜µì…˜ì„ ì˜ë¯¸í•˜ê³  ë‹¤ì–‘í•œ ë°ì´í„° ì „ì²˜ë¦¬ ì „ëµì„ íŠ¸ë¦¬ê±°í•˜ë„ë¡ ì„¤ê³„\n",
    "\n",
    "ğŸ¯ë¯¸ì…˜ 1\n",
    "|Mode|ì „ì²˜ë¦¬ ì°¨ì´ì |ì¥ì |ë‹¨ì |\n",
    "|---|---|---|---|\n",
    "|'all'|ëª¨ë“  ë°ì´í„°ì— ëŒ€í•œ ì¼ê´„ì²˜ë¦¬|ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ í†µí•©ëœ ì „ì²˜ë¦¬ ìˆ˜í–‰|ì¼ë¶€ íŠ¹ì •ë°ì´í„° íŠ¹ì„± ëˆ„ë½ ê°€ëŠ¥ì„±ìˆìŒ|\n",
    "|'byb'|ê±´ë¬¼ë³„ë¡œ ì „ì²˜ë¦¬|ê° ê±´ë¬¼ë§ˆë‹¤ ê°œë³„ì ì¸ ì „ì²˜ë¦¬ ìˆ˜í–‰í•´ ëª¨ë¸ ì„¸ë°€ ì¡°ì • ê°€ëŠ¥|ê±´ë¬¼ë°ì´í„° ë¶€ì¡±í•  ê²½ìš° ëª¨ë¸ì¡°ì •ì— ì–´ë ¤ì›€|\n",
    "|'gu_all'|ê° êµ¬ë³„ë¡œ í…Œì´í„°ë¥¼ ë‚˜ëˆˆ í›„ êµ¬ë³„ë¡œ ì „ì²˜ë¦¬|ê° êµ¬ë³„ë¡œ ì „ì²´ ë°ì´í„°ë¥¼ ë‚˜ëˆ  êµ¬ë³„ë¡œ ì „ì²˜ë¦¬ ìˆ˜í–‰|ê° êµ¬ë³„ì´ ë‹¤ë¥¸ íŠ¹ì„±ì„ ê°–ëŠ” ê²½ìš° ëª¨ë¸ë§ì— ì˜í–¥|\n",
    "|'gu_byb'|ê° êµ¬ë³„ë¡œ ë°ì´í„°ë¥¼ ë‚˜ëˆˆ í›„ ê±´ë¬¼ë³„ë¡œ ì „ì²˜ë¦¬|ê° êµ¬ë³„ë¡œ ê±´ë¬¼ë³„ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•˜ì—¬ êµ¬ë³„, ê±´ë¬¼ë³„ ì¡°ì •|ê° êµ¬ë³„ì´ ë‹¤ë¥¸ íŠ¹ì„±ì„ ê°–ëŠ” ê²½ìš° ëª¨ë¸ë§ì— ì˜í–¥|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcee635-6471-4b19-ba25-f6b54aa059de",
   "metadata": {},
   "source": [
    "# ëª¨ë¸ë§ ê³¼ì • ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3852b7a-2f71-4012-9ebb-cb5a3324df4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê±´ë¬¼ by ê±´ë¬¼ë¡œ í•™ìŠµ\n",
    "gu_byb = ['num_date_time', 'building', 'date_time', 'temp', 'wind', 'hum',\n",
    "       'dow', 'month', 'week', 'dow_hour_mean', 'holiday',\n",
    "       'holiday_mean', 'holiday_std', 'hour_mean', 'hour_std', 'sin_time',\n",
    "       'cos_time', 'THI', 'WC', 'CDH', 'target']\n",
    "\n",
    "# í›ˆë ¨ê³¼ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ê³  ì „ì²˜ë¦¬ / mode == 'gu_byb'ì´ê¸°ì— mergeê°€ ì¼ì–´ë‚˜ì§€ ì•ŠìŒ\n",
    "train, test = get_train_and_test_data('gu_byb')\n",
    "\n",
    "train = train[gu_byb]\n",
    "test = test[gu_byb[:-1]] #target ì œì™¸í•˜ê³  gu_byb ì»¬ëŸ¼ ì„ ì •\n",
    "\n",
    "scores = []\n",
    "best_it = []\n",
    "\n",
    "score = pd.DataFrame({'building':range(1,101)})\n",
    "for i in tqdm(range(100)):\n",
    "    # ê°œë³„ ê±´ë¬¼ì— ëŒ€í•œ x, y ì§€ì •\n",
    "    y = train.loc[train.building == i+1, 'target']\n",
    "    x = train.loc[train.building == i+1, ].iloc[:, 3:].drop(['target'], axis=1)\n",
    "\n",
    "    # ì‹œê³„ì—´ì„±ì„ ê³ ë ¤í•œ validation êµ¬ì¶•\n",
    "    y_train, y_valid, x_train, x_valid = temporal_train_test_split(y = y, X = x, test_size = 168)\n",
    "\n",
    "    xgb = XGBRegressor(colsample_bytree=0.8, eta=0.01, max_depth=5,\n",
    "             min_child_weight=6,n_estimators=2000, subsample=0.9, early_stopping_rounds=50, eval_metric=SMAPE)\n",
    "\n",
    "    xgb.set_params(**{'objective':weighted_mse(100)})\n",
    "\n",
    "    xgb.fit(x_train, y_train, eval_set=[(x_train, y_train),\n",
    "                                            (x_valid, y_valid)], verbose=False)\n",
    "\n",
    "    y_pred = xgb.predict(x_valid)\n",
    "\n",
    "    sm = SMAPE(y_valid, y_pred)\n",
    "    scores.append(sm)\n",
    "    best_it.append(xgb.best_iteration+1) # ê° ê±´ë¬¼ë“¤ì— ëŒ€í•œ ìµœì  ë°˜ë³µíšŸìˆ˜\n",
    "\n",
    "score['score'] = scores\n",
    "print(sum(scores)/len(scores)) # í‰ê·  ì ìˆ˜\n",
    "print(sum(best_it)/len(best_it)) # í‰ê·  ìµœì  ë°˜ë³µíšŸìˆ˜\n",
    "# 4.404954637397799\n",
    "# 451.67\n",
    "\n",
    "# ì •ë‹µì„ ë‹´ì•„ë‘ê¸° ìœ„í•œ ë°°ì—´ ìƒì„±\n",
    "preds = np.array([])\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    pred_df = pd.DataFrame()\n",
    "\n",
    "    # ê° ê±´ë¬¼ì— ëŒ€í•´ì„œ seed 0~4ì¸ ê²½ìš°ë¥¼ ë°˜ë³µí•´ì„œ í•™ìŠµ - ì¶”ë¡ \n",
    "    for seed in [0,1,2,3,4]:\n",
    "        # ê±´ë¬¼ë³„ train xì™€ y, test êµ¬ì¶•\n",
    "        y_train = train.loc[train.building == i+1, 'target']\n",
    "        x_train = train.loc[train.building == i+1, ].iloc[:, 3:].drop(['target'], axis=1)\n",
    "        x_test = test.loc[test.building == i+1, ].iloc[:,3:]\n",
    "\n",
    "        # ê° ê±´ë¬¼ì— ëŒ€í•œ ìµœì  ë°˜ë³µíšŸìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "        xgb = XGBRegressor(colsample_bytree=0.8, eta=0.01, max_depth=5, seed=seed,\n",
    "                 min_child_weight=6,n_estimators=best_it[i], subsample=0.9)\n",
    "\n",
    "        xgb.fit(x_train, y_train)\n",
    "        y_pred = xgb.predict(x_test)\n",
    "        pred_df.loc[:,seed] = y_pred # ì¶”ë¡  ê²°ê³¼ë¥¼ ë°°ì—´ì— ì €ì¥ (seedê°’ì´ ì—´ë¡œ ì¡´ì¬)\n",
    "\n",
    "    # ë°°ì—´ì˜ ê°™ì€ í–‰ì— ì¡´ì¬í•˜ëŠ” seedë³„ ì˜ˆì¸¡ê°’ì˜ í‰ê· \n",
    "    pred = pred_df.mean(axis=1)\n",
    "\n",
    "    # seedë³„ í‰ê· ê°’(pred)ì„ preds ë°°ì—´ì— ì—…ë°ì´íŠ¸ (ê±´ë¬¼ 0, 1, ... 99, 100 ìˆœì„œ ì¦‰, sample_submission ìˆœì„œì™€ ë™ì¼)\n",
    "    preds = np.append(preds, pred)\n",
    "\n",
    "submission = pd.read_csv(os.path.join(base_path,'sample_submission.csv'))\n",
    "submission['answer'] = preds\n",
    "submission.to_csv('./gu_byb.csv', index = False) # ê±´ë¬¼ë³„ ì˜ˆì¸¡ì„ gu_bybë¡œ ì €ì¥\n",
    "\n",
    "del train, test, scores, best_it, submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c2e3f7-34cf-4477-a795-94bf150c42aa",
   "metadata": {},
   "source": [
    "**mode=gu_byb**\n",
    "ì½”ë“œ ì„¤ëª…\n",
    "- gu_byb modeë¡œ get_train_and_test_data()í•˜ì—¬ ë°ì´í„° ë³‘í•©ì´ ì§„í–‰ë˜ì§€ ì•ŠìŒ.\n",
    "- `train = train[gu_byb]`: gu_bybì— ì •ì˜ëœ ì»¬ëŸ¼ë“¤ë§Œ ì„ íƒí•´ í›ˆë ¨ë°ì´í„°ë¡œ ì‚¬ìš©\n",
    "- `test = test[gu_byb[:-1]]`: gu_bybë¦¬ìŠ¤íŠ¸ì—ì„œ ë§ˆì§€ë§‰ ìš”ì†Œ(target)ë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ì„ íƒ\n",
    "\n",
    "    - scores: ëª¨ë¸í‰ê°€ì ìˆ˜(SMAPE) ì €ì¥, best_it: ê±´ë¬¼ì— ëŒ€í•œ ìµœì  ë°˜ë³µíšŸìˆ˜\n",
    "    - íƒ€ê²Ÿë³€ìˆ˜ë¥¼ ì™œ ì´ë ‡ê²Œ ì„¤ì •í–ˆëŠ”ì§€ ì•„ì§ ì˜ë¬¸..\n",
    "    - XGBRegressor(colsample_bytree:ì„ íƒí•  íŠ¹ì„±ë¹„ìœ¨, eta:í•™ìŠµë¥ , max_depth:íŠ¸ë¦¬ì˜ê¹Šì´, min_child_weight: ìì‹íŠ¸ë¦¬ì— í•„ìš”í•œ ìµœì†Œ ê°€ì¤‘ì¹˜ì˜ í•©, n_estimators: íŠ¸ë¦¬ì˜ìˆ˜, subsample: í•™ìŠµ ë°ì´í„°ì…‹ ì‚¬ìš©í•  ë¹„ìœ¨,early_stopping_rounds:ì¡°ê¸°ì¤‘ì§€, eval_metric: ê²€ì¦ë°ì´í„° í‰ê°€ì§€í‘œ ë‚˜íƒ€ë‚´ëŠ” íŒŒë¼ë¯¸í„°)\n",
    "    - xgb.set_params(**{'objective':weighted_mse(100)}): í‰ê· ì œê³± ì˜¤ì°¨ì‚¬ìš©í•˜ê³  ê°€ì¤‘ì¹˜ëŠ” 100ìœ¼ë¡œ ì„¤ì •ëœ ëª©ì í•¨ìˆ˜ ì‚¬ìš©\n",
    "    - fit: ëª¨ë¸í•™ìŠµ, eval_setíŒŒë¼ë¯¸í„° í†µí•´ ê²€ì¦ë°ì´í„° ì§€ì •í•˜ê³  í›ˆë ¨ ê³¼ì •ì€ ì¶œë ¥í•˜ì§€ì•ŠìŒ\n",
    "    - predict: ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "    - SMAPE: ëª¨ë¸ì˜ ì˜ˆì¸¡ê³¼ ì‹¤ì œê°’ ì‚¬ì´ í‰ê·  ì ˆëŒ€ ë°±ë¶„ìœ¨ ì˜¤ì°¨ë¥¼ ë‚˜íƒ€ëƒ„\n",
    "    - best_iterationì€ ëª¨ë¸ì´ ìë™ìœ¼ë¡œ ì„ íƒí•œ ìµœì ì˜ ë°˜ë³µíšŸìˆ˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a8b445-b3ea-4ef4-a7c3-0e7a3796649f",
   "metadata": {},
   "source": [
    "> ê° ê±´ë¬¼ì— ëŒ€í•´ ë°˜ë³µí•´ í•´ë‹¹ ê±´ë¬¼ì— ëŒ€í•œ íŠ¹ì„±ê³¼ íƒ€ê²Ÿì„ ì¶”ì¶œí•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•œë‹¤. ì‹œê³„ì—´ì„±ì„ ê³ ë ¤í•´ ê²€ì¦ë°ì´í„° ë¶„í• í•˜ê³  XGBoostíšŒê·€ëª¨ë¸ì„ ì„¤ì •í•˜ê³  í•™ìŠµí•¨. í•™ìŠµëœ ëª¨ë¸ë¡œ ê²€ì¦ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ ìˆ˜í–‰í•˜ê³  í‰ê°€ì§€í‘œ ê³„ì‚°í•´ ì„±ëŠ¥ì„ í‰ê°€í•œë‹¤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e09b2d-e189-444d-93e9-826cd22e6038",
   "metadata": {},
   "source": [
    "100ê°œì˜ ê±´ë¬¼ì— ëŒ€í•´ ë°˜ë³µí•˜ì—¬ ê° ê±´ë¬¼ë³„ ì˜ˆì¸¡ì„ ìˆ˜í–‰\n",
    "seedê°’ì„ 0~4ê¹Œì§€ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ì´ˆê¸°í™” í•˜ê³  í•™ìŠµ -> ì´ë¥¼ í†µí•´ ë‹¤ì–‘í•œ ì´ˆê¸°í™” ì¡°ê±´ìœ¼ë¡œ í•™ìŠµí•˜ê³  ë‹¤ì–‘ì„±ì„ í™œìš©í•´ ê°•ê±´í•œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ë„ë¡ í•œë‹¤.\n",
    "  - y_train, x_trainì˜ buildingì„ ì™œ i+1ë¡œ ì²˜ë¦¬í•œì§€ëŠ” ì˜ë¬¸, y_trainì€ í•´ë‹¹ê±´ë¬¼ì˜ targetì„ ì˜ˆì¸¡í•˜ë ¤ëŠ” ê°’\n",
    "  - x_train: ê±´ë¬¼(i+1)ì— ëŒ€í•œ í•™ìŠµë°ì´í„°(íŠ¹ì§•), í•„ìš”í•œ ì—´(3ë²ˆì§¸ì—´ë¶€í„° ë§ˆì§€ë§‰ ì—´)ì„ ì‚¬ìš©í•´ ë°ì´í„° ì„ íƒí›„ targetì œê±°í›„ ì‚¬ìš©\n",
    "  - x_test: í•„ìš”í•œ ì—´ë§Œ ì‚¬ìš©\n",
    "  - ëª¨ë¸ì€ XGBoost ì‚¬ìš©\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1183474a-e871-47b8-91b5-17fc97797276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# ì „ì²´ ê±´ë¬¼ë¡œ í•™ìŠµ\n",
    "gu_all = ['num_date_time', 'building', 'date_time', 'temp', 'wind', 'hum',\n",
    "       'type', 'all_area', 'cool_area', 'dow', 'month', 'week',\n",
    "       'dow_hour_mean', 'date', 'holiday', 'holiday_mean', 'holiday_std',\n",
    "       'hour_mean', 'hour_std', 'sin_time', 'cos_time', 'THI', 'WC', 'CDH', 'target']\n",
    "\n",
    "train, test = train[gu_all], test[gu_all[:-1]]\n",
    "\n",
    "train['date'] = pd.to_datetime(train['date'])\n",
    "train['building'] = train['building'].astype('category')\n",
    "train['type'] = train['type'].astype('category')\n",
    "\n",
    "x_train = train[train['date'] < f'2022-08-18'].drop(['num_date_time', 'date_time', 'target', 'date'], axis=1).reset_index(drop=True)\n",
    "x_valid = train[train['date'] >= f'2022-08-18'].drop(['num_date_time', 'date_time', 'target', 'date'], axis=1).reset_index(drop=True)\n",
    "y_train = train[train['date'] < f'2022-08-18']['target'].reset_index(drop=True)\n",
    "y_valid = train[train['date'] >= f'2022-08-18']['target'].reset_index(drop=True)\n",
    "\n",
    "dtrain = xgb.DMatrix(data=x_train, label=y_train, enable_categorical=True)\n",
    "dvalid = xgb.DMatrix(data=x_valid, label=y_valid, enable_categorical=True)\n",
    "\n",
    "#dfì—ì„œ ê° ì—´ì˜ ì²« ê°’ì„ ê°€ì ¸ì˜´\n",
    "param = {\n",
    "    'reg_lambda': df['params_reg_lambda'][0] ,\n",
    "    'gamma': df['params_gamma'][0],\n",
    "    'reg_alpha': df['params_reg_alpha'][0] ,\n",
    "    'colsample_bytree': df['params_colsample_bytree'][0] ,\n",
    "    'subsample': df['params_subsample'][0] ,\n",
    "    'max_depth': df['params_max_depth'][0],\n",
    "    'min_child_weight': df['params_min_child_weight'][0],\n",
    "}\n",
    "\n",
    "model = xgb.train(params=param, dtrain=dtrain, num_boost_round=1000,\n",
    "                  evals=[(dvalid, 'valid')], early_stopping_rounds=100, verbose_eval=False)\n",
    "\n",
    "preds = model.predict(dvalid)\n",
    "smape = SMAPE(y_valid, preds)\n",
    "\n",
    "best_it = model.best_iteration+1\n",
    "building_score = []\n",
    "\n",
    "# ê±´ë¬¼ë§ˆë‹¤ì˜ validation SMAPE scoreë¥¼ ë„ì¶œ\n",
    "for i in range(100):\n",
    "    building_score.append(SMAPE(y_valid[i*168:(i+1)*168], preds[i*168:(i+1)*168]))\n",
    "\n",
    "# í•´ë‹¹ ê²°ê³¼ëŠ” score.csvì— ì €ì¥\n",
    "score['score_all'] = building_score\n",
    "score.to_csv('./score.csv', index=False)\n",
    "print(smape, best_it)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ca371f-9147-41f5-846a-8be4750c0a84",
   "metadata": {},
   "source": [
    "**mode=gu_all**\n",
    "building_infoë¡œ ë³‘í•©ëœ ëª¨ë“  ë°ì´í„°ë“¤ì„ ì‚¬ìš©\n",
    " - ë¹Œë”©ê³¼ íƒ€ì… ì—´ì€ ì¹´í…Œê³ ë¦¬í˜• ë³€ìˆ˜ë¡œ ë³€í™˜, dateì—´ì€ ë‚ ì§œ ì •ë³´ë¡œ ë³€í™˜\n",
    " - x_train, x_valid íŠ¹ì • ë‚ ì§œ(2022-08-18)ì´ì „ê³¼ ì´í›„ë¡œ ë°ì´í„° ë¶„í• (XGBoostëª¨ë¸ í•™ìŠµê³¼ ê²€ì¦ì— ì‚¬ìš©)\n",
    " - y_train, y_valid íŠ¹ì • ë‚ ì§œ(2022-08-18)ì´ì „ê³¼ ì´í›„ì˜ tartgetê°’ ì¶”ì¶œ(ëª©í‘¯ê°’ìœ¼ë¡œ ëª¨ë¸í•™ìŠµê³¼ ê²€ì¦í•´ ì„±ëŠ¥í‰ê°€)\n",
    " - dtrain, dvalid: XGBoostëª¨ë¸ì— í•„ìš”í•œ DMatrixí˜•ì‹ìœ¼ë¡œ ë°ì´í„° ë³€í™˜\n",
    "(ì™œ ì—¬ê¸°ì„œë§Œ DMatrixí˜•ì‹ìœ¼ë¡œ ë°”ê¿¨ì„ê¹Œ? ì—¬ì „íˆ matrixí˜•ì‹ì´ì˜€ì„ ê²ƒ ê°™ì€ë°?..ë¼ëŠ” ì˜ë¬¸)\n",
    " - XGBoostëª¨ë¸ì„ í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cc69de-a2a4-4e50-9cca-7a78829e300c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê±´ë¬¼ by ê±´ë¬¼ë¡œ í•™ìŠµ\n",
    "# pdf p18 : V2ì˜ 3ë²ˆ\n",
    "# ë” ë§ì€ featureë¥¼ ì‚¬ìš©, Feature ê°€ì¤‘ì¹˜ ì¡°ì •, í™•ì‹¤í•œ holidayë§Œ ë§ˆí‚¹\n",
    "byb = ['num_date_time', 'building', 'date_time', 'temp', 'wind', 'hum',\n",
    "       'dow', 'month', 'week', 'dow_hour_mean', 'holiday',\n",
    "       'holiday_mean', 'holiday_std', 'hour_mean', 'hour_std', 'sin_time',\n",
    "       'cos_time', 'THI', 'WC', 'CDH', 'summer_cos', 'summer_sin', 'target']\n",
    "\n",
    "# í›ˆë ¨ê³¼ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ê³  ì „ì²˜ë¦¬ / mode == 'byb'ì´ê¸°ì— merge ë°œìƒí•˜ì§€ ì•ŠìŒ\n",
    "train, test = get_train_and_test_data('byb')\n",
    "train, test = train[byb], test[byb[:-1]]\n",
    "\n",
    "scores = []\n",
    "best_it = []\n",
    "\n",
    "for b in tqdm(range(100)):\n",
    "    y = train.loc[train.building == b+1, 'target']\n",
    "    x = train.loc[train.building == b+1, ].iloc[:, 3:].drop(['target'], axis=1)\n",
    "    y_train, y_valid, x_train, x_valid = temporal_train_test_split(y = y, X = x, test_size = 168)\n",
    "\n",
    "    xgb = XGBRegressor(colsample_bytree=0.8, eta=0.1, max_depth=5,\n",
    "         min_child_weight=6,n_estimators=1000, subsample=0.9, early_stopping_rounds=50)\n",
    "\n",
    "    xgb.fit(x_train, y_train, eval_set=[(x_valid, y_valid)], verbose=False)\n",
    "\n",
    "    y_pred = xgb.predict(x_valid)\n",
    "\n",
    "\n",
    "    sm = SMAPE(y_valid, y_pred)\n",
    "    scores.append(sm)\n",
    "    best_it.append(xgb.best_iteration+1)\n",
    "\n",
    "print(sum(scores)/len(scores))\n",
    "print(sum(best_it)/len(best_it))\n",
    "# 5.17304185151095\n",
    "# 107.93\n",
    "\n",
    "# 11ê°œ seedì— ëŒ€í•´ì„œ ê±´ë¬¼ë³„ ìµœì  ë°˜ë³µíšŸìˆ˜ë¡œ ì˜ˆì¸¡ ì‚¬í–‰\n",
    "preds = np.array([])\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    pred_df = pd.DataFrame()\n",
    "\n",
    "    for seed in [0,1,2,3,4,5,6,7,8,9,10]:\n",
    "        y_train = train.loc[train.building == i+1, 'target']\n",
    "        x_train = train.loc[train.building == i+1, ].iloc[:, 3:].drop(['target'], axis=1)\n",
    "        x_test = test.loc[test.building == i+1, ].iloc[:,3:]\n",
    "\n",
    "        xgb = XGBRegressor(colsample_bytree=0.8, eta=0.1, max_depth=5, seed=seed,\n",
    "             min_child_weight=6,n_estimators=best_it[i], subsample=0.9)\n",
    "\n",
    "        xgb.fit(x_train, y_train)\n",
    "        y_pred = xgb.predict(x_test)\n",
    "        pred_df.loc[:,seed] = y_pred\n",
    "\n",
    "    # ì˜ˆì¸¡ ê²°ê³¼ í‰ê· ìœ¼ë¡œ ì•™ìƒë¸”\n",
    "    pred = pred_df.mean(axis=1)\n",
    "    preds = np.append(preds, pred)\n",
    "\n",
    "submission = pd.read_csv('open/sample_submission.csv')\n",
    "submission['answer'] = preds\n",
    "submission.to_csv('./byb.csv', index = False)\n",
    "submission\n",
    "\n",
    "del train, test, scores, best_it, submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2c92b7-697e-4387-83b3-ed8d56fc797f",
   "metadata": {},
   "source": [
    "**mode=byb**\n",
    "mergeí•˜ì§€ ì•Šê³  featureê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
    " 1. ê±´ë¬¼ ë³„ ëª¨ë¸í•™ìŠµ\n",
    "   - 100ê°œì˜ ê±´ë¬¼ì— ëŒ€í•´ ë°˜ë³µ\n",
    "   - ê° ê±´ë¬¼ì— ëŒ€í•œ í›ˆë ¨ ë° ê²€ì¦ë°ì´í„° ìƒì„±\n",
    "   - XGBoostëª¨ë¸ ì‚¬ìš©í•´ í•™ìŠµ\n",
    "   - í•™ìŠµ ì¤‘, early_stopping_roundsì„ ì‚¬ìš©í•´ ì¡°ê¸° ì¤‘ë‹¨ ì¡°ê±´ ì„¤ì •\n",
    "   - ê²€ì¦ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ ìƒì„±í›„ SMAPEì ìˆ˜ ê³„ì‚°\n",
    "\n",
    " 2. ê±´ë¬¼ ë³„ ìµœì  ë°˜ë³µ íšŸìˆ˜ ì˜ˆì¸¡\n",
    "   - ê° ê±´ë¬¼ì— ëŒ€í•´ 11ê°œì˜ ë‹¤ë¥¸ seedê°’ ì ìš©í•˜ì—¬ ìµœì  ë°˜ë³µíšŸìˆ˜ ì˜ˆì¸¡\n",
    "   - ê° seedì— ëŒ€í•´ ëª¨ë¸ ë‹¤ì‹œ í•™ìŠµ í›„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ ìƒì„±\n",
    " 3. ì•™ìƒë¸” ë° ê²°ê³¼ ì œì¶œ\n",
    "   - 11ê°œì˜ ì˜ˆì¸¡ì„ í‰ê· í•˜ì—¬ ìµœì¢…ì˜ˆì¸¡ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df94bebe-2a28-467f-9af8-3f4176b67f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# ì „ì²´ ê±´ë¬¼ë¡œ í•™ìŠµ (pdfì—ì„œ ë§í•œ ê²ƒê³¼ ë‹¤ë¥´ë‚˜ ì½”ë“œìƒìœ¼ë¡œëŠ” ì „ì²´ ê±´ë¬¼ë¡œ í•™ìŠµ)\n",
    "# pdf p18 : V2ì˜ 4ë²ˆ\n",
    "# ë” ë§ì€ featureë¥¼ ì‚¬ìš©, Feature ê°€ì¤‘ì¹˜ ì¡°ì •, í™•ì‹¤í•œ holidayë§Œ ë§ˆí‚¹\n",
    "_all = ['num_date_time', 'building', 'date_time', 'temp', 'prec', 'wind', 'hum',\n",
    "       'type', 'all_area', 'cool_area', 'sun', 'dow', 'month',\n",
    "       'week', 'avg_temp', 'max_temp', 'min_temp', 'temp_diff',\n",
    "       'dow_hour_mean', 'holiday', 'holiday_mean', 'holiday_std',\n",
    "       'hour_mean', 'hour_std', 'sin_time', 'cos_time', 'THI', 'WC', 'CDH', 'target']\n",
    "\n",
    "train, test = get_train_and_test_data('all')\n",
    "train, test = train[_all], test[_all[:-1]]\n",
    "\n",
    "train['date'] = pd.to_datetime(train['date_time'])\n",
    "train['building'] = train['building'].astype('category')\n",
    "train['type'] = train['type'].astype('category')\n",
    "\n",
    "x_train = train[train['date'] < '2022-08-18'].drop(['num_date_time', 'date_time', 'target', 'date'], axis=1)\n",
    "x_valid = train[train['date'] >= '2022-08-18'].drop(['num_date_time', 'date_time', 'target', 'date'], axis=1)\n",
    "y_train = train[train['date'] < '2022-08-18']['target']\n",
    "y_valid = train[train['date'] >= '2022-08-18']['target']\n",
    "\n",
    "dtrain = xgb.DMatrix(data=x_train, label=y_train, enable_categorical=True)\n",
    "dvalid = xgb.DMatrix(data=x_valid, label=y_valid, enable_categorical=True)\n",
    "\n",
    "param = {\n",
    "    'reg_lambda': df['params_reg_lambda'][0] ,\n",
    "    'gamma': df['params_gamma'][0],\n",
    "    'reg_alpha': df['params_reg_alpha'][0] ,\n",
    "    'colsample_bytree': df['params_colsample_bytree'][0] ,\n",
    "    'subsample': df['params_subsample'][0] ,\n",
    "    'max_depth': df['params_max_depth'][0],\n",
    "    'min_child_weight': df['params_min_child_weight'][0],\n",
    "    'eta' : 0.1,\n",
    "}\n",
    "\n",
    "model = xgb.train(params=param, dtrain=dtrain, num_boost_round=1000,\n",
    "                  evals=[(dvalid, 'valid')], early_stopping_rounds=50, verbose_eval=False)\n",
    "\n",
    "preds = model.predict(dvalid)\n",
    "\n",
    "smape = SMAPE(y_valid, preds)\n",
    "\n",
    "score = smape\n",
    "best_it = model.best_iteration+1 # ì „ì²´ ê±´ë¬¼ë¡œ í•™ìŠµí–ˆì„ ë•Œ ìµœì  iterationê°’ ì €ì¥\n",
    "print(score, best_it) # 4.946742493265771 [844]\n",
    "\n",
    "preds = np.array([])\n",
    "\n",
    "test['date'] = pd.to_datetime(test['date_time'])\n",
    "test['building'] = test['building'].astype('category')\n",
    "test['type'] = test['type'].astype('category')\n",
    "\n",
    "x_train = train.drop(['num_date_time', 'date_time', 'target', 'date'], axis=1)\n",
    "x_test = test.drop(['num_date_time', 'date_time', 'date'], axis=1)\n",
    "y_train = train['target']\n",
    "\n",
    "dtrain = xgb.DMatrix(data=x_train, label=y_train, enable_categorical=True)\n",
    "dtest = xgb.DMatrix(data=x_test, enable_categorical=True)\n",
    "\n",
    "pred_df = pd.DataFrame()\n",
    "\n",
    "i = 0\n",
    "for seed in tqdm(range(5)):\n",
    "    # seed 0~4ì— ëŒ€í•´ì„œ ì‹¤í–‰\n",
    "    param = {\n",
    "        'reg_lambda': df['params_reg_lambda'][0] ,\n",
    "        'gamma': df['params_gamma'][0],\n",
    "        'reg_alpha': df['params_reg_alpha'][0] ,\n",
    "        'colsample_bytree': df['params_colsample_bytree'][0] ,\n",
    "        'subsample': df['params_subsample'][0] ,\n",
    "        'max_depth': df['params_max_depth'][0],\n",
    "        'min_child_weight': df['params_min_child_weight'][0],\n",
    "        'seed':seed,\n",
    "        'eta':0.1\n",
    "    }\n",
    "\n",
    "    # ì „ì— ì €ì¥í•´ë‘” ìµœì  iteration - 100ë§Œí¼ num_boost_round ì‹¤í–‰\n",
    "    model = xgb.train(params=param, dtrain=dtrain, num_boost_round=best_it-100)\n",
    "\n",
    "    y_pred = model.predict(dtest)\n",
    "    pred_df.loc[:,seed] = y_pred\n",
    "\n",
    "pred = pred_df.mean(axis=1)\n",
    "preds = np.append(preds, pred)\n",
    "\n",
    "submission = pd.read_csv('open/sample_submission.csv')\n",
    "submission['answer'] = preds\n",
    "submission.to_csv('./all.csv', index = False)\n",
    "\n",
    "del train, test\n",
    "del dtrain, dtest, x_train, x_test, y_train, pred, preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b41571a-4b00-4e21-960d-f5b88febdb68",
   "metadata": {},
   "source": [
    "**mode==all**\n",
    "2022-08-18'ì´ì „ì€ í›ˆë ¨ë°ì´í„°, ì´í›„ëŠ” ê²€ì¦ë°ì´í„°ë¡œ ì‚¬ìš©\n",
    "1. ë°ì´í„° ì „ì²˜ë¦¬: ë‚ ì§œì™€ ê±´ë¬¼ì •ë³´ë¥¼ ì¹´í…Œê³ ë¦¬ í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "2. XGBoostëª¨ë¸í•™ìŠµ: ì„¤ì •í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì™€ í•¨ê»˜ í•™ìŠµ\n",
    "3. ëª¨ë¸í‰ê°€: ê²€ì¦ë°ì´í„°ì— ëŒ€í•œ SMAPEì ìˆ˜ì™€ ìµœì  ë°˜ë³µ íšŸìˆ˜ë¡œ ì €ì¥\n",
    "\n",
    "... ë‚˜ë¨¸ì§€ ë™ì¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acec966-a615-43d1-a44a-608a86ac6f27",
   "metadata": {},
   "source": [
    "[ë¯¸ì…˜2] ratioì˜ ì—­í• ì´ ì™œ êµ³ì´ ìˆì—ˆì„ê¹Œ ë¼ëŠ” ìƒê°ê³¼ ì™œ ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ ì œì¶œì„ í• ë•Œ ê³±í•´ì„œ ì €ì¥í•˜ëŠ”ì§€ ì•„ì§ ì´í•´ê°€ ì˜ ë˜ì§€ì•Šì•˜ìŒ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ecaedd-9e90-420e-8bb0-afaa359594c4",
   "metadata": {},
   "source": [
    "ğŸ¯ë¯¸ì…˜ 3\n",
    "\n",
    "ì´ìœ )\n",
    "1. seedê°’ì€ ì•Œê³ ë¦¬ì¦˜ì˜ ë¬´ì‘ìœ„ì„± ì œì–´ì— ì‚¬ìš©. XGBoostì™€ ê°™ì€ ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì€ ì´ˆê¸°í™” ì‹œ ë¬´ì‘ìœ„ì„±ì„ í¬í•¨í•˜ê³  ìˆì–´ ê²°ê³¼ê°€ ë‹¤ë¥´ê²Œ ë‚˜ì˜¬ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ Randomnessì œì–´ë¥¼ ìœ„í•´ ì‚¬ìš©ë˜ì—ˆìŒì„ ë³´ì¸ë‹¤.\n",
    "2. ë‹¤ë¥¸ ì‹œë“œê°’ì—ì„œ í•™ìŠµí•œ ì—¬ëŸ¬ ëª¨ë¸ì˜ í‰ê· ì˜ˆì¸¡ì€ ë‹¨ì¼ ëª¨ë¸ì˜ ì˜ˆì¸¡ë³´ë‹¤ ì•ˆì •ì ì´ë‹¤.\n",
    "     \n",
    "ê¸°ëŒ€íš¨ê³¼)\n",
    "1. ì•™ìƒë¸”ê³¼ ê°™ì€ íš¨ê³¼ë¥¼ ë‚˜íƒ€ëƒ„\n",
    "2. ë‹¤ì–‘í•œ ì´ˆê¸°í™”ì—ì„œ ì–»ì€ ê²°ê³¼ì˜ í‰ê· ì€ ëª¨ë¸ì´ ë‹¤ì–‘í•œ íŠ¹ì§•ì„ í•™ìŠµí•˜ê³  ë°ì´í„°ì˜ ë…¸ì´ì¦ˆì— ë¯¼ê°í•˜ì§€ ì•Šê²Œ ë§Œë“¤ì–´ ëª¨ë¸ ì„±ëŠ¥ì— í–¥ìƒì„ ê¸°ëŒ€í•  ìˆ˜ ìˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e087b32b-9057-4159-a869-638d20bfc58e",
   "metadata": {},
   "source": [
    "enable_categorical ë§¤ê°œ ë³€ìˆ˜ ì„¤ì •ì´ í•˜ëŠ” ì—­í• )\n",
    "\n",
    "  `enable_categorical`ëŠ” XGBoostëª¨ë¸ì—ì„œ ë²”ì£¼í˜•(categorical) íŠ¹ì§•ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ì˜µì…˜ì„ ì„¤ì •í•  ë•Œ ì‚¬ìš©.\n",
    "  ë²”ì£¼í˜•ì˜ íŠ¹ì§•ì€ ë¬¸ìì—´ ë˜ëŠ” ì •ìˆ˜ë¡œ í‘œì‹œë˜ì–´ ìˆë‹¤.\n",
    "  enable_categorical=Trueë¡œ ì„¤ì •í•˜ëŠ” ê²ƒì€ XGBoostì—ê²Œ íŠ¹ì • ì—´ì´ ë²”ì£¼í˜•ì„ì„ ì•Œë ¤ì£¼ëŠ” ì—­í• ì„ í•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dde2f6b-b437-49be-8ca2-8fcb97b0d08f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
